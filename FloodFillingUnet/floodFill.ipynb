{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitmlgpuconda09ab47ec0af1474a87a3758d948ae547",
   "display_name": "Python 3.7.7 64-bit ('MLGPU': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Flood Fill Strategy for Instance Segmentation\n",
    "\n",
    "The Flood Filling approach demonstrated by [Januszewski et al.](https://arxiv.org/abs/1611.00421) does not strictly depend on the underlying architecture of the FCCN used. Here we try to adapt the implented 3D Unet to be trained and used as a flood filling network.\n",
    "\n",
    "Flood Filling networks actively construct a segmentation mask that is fed into the network together with the input image. This results in a quasi recurrent architecture that can incorporate previous predictions made for the border regions.\n",
    "\n",
    "In inference mode the flood filling algorithm works as follows:\n",
    "- Retrieve a Tile from the top of the working queue and ensure that is has not been visited before\n",
    "- Evaluate the FCCN on a 3D Tile of training data and the segmentation canvas\n",
    "- Write the FCCN ouput to the segmentation canvas\n",
    "- Evaluate in which directions the object extends and add the respective tiles to a queue.\n",
    "\n",
    "For training the segmentation canvas is initialy blank except for a single seed withing the object that is currently segmented. The training volume allows to jump at least to imediately neighbouring tiles and continue segmentation with the previous prediction before a new scene is loaded.\n",
    "\n",
    "The following tasks need to be adresses in order to implement the flood filling approach:\n",
    "\n",
    "Stage 1: Single object segmentation\n",
    "The first intermediate goal is to segment single neuron data using the flood filling approach. This potentially reduces the number of tiles that need to be evaluated since the network explores the neuron processes. \n",
    "\n",
    "- Adapt the Unet to expect two channel input (image and segmentation mask)\n",
    "- Write a class that \n",
    "  - tiles the image with overlapping tiles, \n",
    "  - maintains the segmentation mask, \n",
    "  - delivers appropriate slices of image and segmentation mask, \n",
    "  - maintains a tile coordinate queue that should be visited and \n",
    "  - prevents reevaluation of visited sites\n",
    "- Write a custom training script that feeds a seeded segmentation mask and a small region of the image for training\n",
    "- Write a custom evaluation script that executes segmentation of large volumes by running the segmentation loop over longer time\n",
    "\n",
    "Stage 2: Multi Object Segmentation\n",
    "In a next step image data and ground truth for multi neuron situations should be generated and used to train the FCCN so that it learns to ignore signals that are not adjacent to previously masked regions -> generalizes the network for instance segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os, time\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from importlib import reload\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules providing tools for image manipulation\n",
    "import sys\n",
    "sys.path.append('../tools')\n",
    "import tilingStrategy, Dataset3D, visualization, metrics\n",
    "sys.path.append('../3D Unet')\n",
    "import model\n",
    "import floodFill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1Physical GPUs,1Logical GPUs\n"
    }
   ],
   "source": [
    "# Fix for tensorflow-gpu issues that I found online... (don't ask me what it does)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input Pipeline for FFUnet Training\n",
    "\n",
    "The training data is stored in a hdf5 file created with Dataset3D. \n",
    "Each entry contains a training scene that fits a grid of 3x3x3 evaluation positions for a given input and output shape together with the stepsize.\n",
    "The scenes are loaded and preprocessed using tf data. During one epoch all scenes are used once.\n",
    "From each scene, a FloodFiller instance is created that maintains a new processing queue and a segmentation canvas. The unet is trained on the scene until the queue of the FloodFiller is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Opened dataset with 48 preexisting items. Overwriting items with the same name.\nDataset <Dataset3D.Dataset object at 0x0000029339E09D88> contains 48 records\nDataset metadata entries:\n<KeysViewHDF5 ['background_weight', 'comment', 'input_shape', 'object_weight', 'stepsize']>\nbackground_weight : 1.0232130589855122\ncomment : This dataset is intended for flood filling network training. Each training example allows for a grid of 3x3x3 evaluations. Each example is guaranteed to contain the object in the central tile\ninput_shape : [220 220 220]\nobject_weight : 44.07919954126329\nstepsize : [8 8 8]\n"
    }
   ],
   "source": [
    "# Load images and masks from a previously created dataset\n",
    "#base_dir = 'C:\\\\Users\\\\lillvisj\\\\linus\\\\'\n",
    "base_dir = 'C:\\\\Users\\\\Linus Meienberg\\\\Documents\\\\ML Datasets\\\\FruSingleNeuron_20190707\\\\'\n",
    "dataset_path = base_dir+'ffn_dataset.h5'\n",
    "dataset = Dataset3D.Dataset(dataset_path) # The Dataset3D class handles all file level i/o operations\n",
    "\n",
    "print('Dataset {} contains {} records'.format(dataset, len(dataset)))\n",
    "\n",
    "print('Dataset metadata entries:')\n",
    "print(dataset.getAttributes().keys())\n",
    "for key in dataset.getAttributes().keys():\n",
    "    print(key + ' : ' + str(dataset.getAttributes().get(key)))\n",
    "\n",
    "#Maybe look at en entry\n",
    "#print(dataset.keys())\n",
    "im, msk, meta = dataset.get(122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE UNET GEOMETRY\n",
    "input_shape = (220,220,220)\n",
    "delta = (8,8,8)\n",
    "# with two downsampling blocks this implies\n",
    "output_shape = (132,132,132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all records in the database\n",
    "entries = list(dataset.keys())\n",
    "\n",
    "# Make a train test split and retrieve a callable -> that produces a generator -> that yields the recods specified by the key list in random order\n",
    "training = dataset.getGenerator(entries[:30])\n",
    "test = dataset.getGenerator(entries[:30])\n",
    "\n",
    "# gen = training() # calling the callable training produces a generator\n",
    "# next(gen) -> (image, mask) yields the records identified by the key list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tf Datasets from the generator producing callables, specify the datatype and shape of the generator output\n",
    "trainingset_raw = tf.data.Dataset.from_generator(training, \n",
    "    output_types=(tf.float32, tf.int32),\n",
    "    output_shapes=(tf.TensorShape([236,236,236]),tf.TensorShape([236,236,236])))\n",
    "\n",
    "testset_raw = tf.data.Dataset.from_generator(test, \n",
    "    output_types=(tf.float32, tf.int32),\n",
    "    output_shapes=(tf.TensorShape([236,236,236]),tf.TensorShape([236,236,236])))\n",
    "\n",
    "# each scene is preprocessed by passing it through this function\n",
    "def preprocessScene(x,y):\n",
    "    # clip, and z normalize image\n",
    "    x = tf.clip_by_value(x, 0, 1400)\n",
    "    x = tf.subtract(x, 140)\n",
    "    x = tf.divide(x, 40)\n",
    "    # binarize mask\n",
    "    y = tf.clip_by_value(y,0,1)\n",
    "   \n",
    "    return x, y\n",
    "\n",
    "# Each Tile that is provided by a Flood Filler is passed\n",
    "def preprocessTile(image, segmentation, mask):\n",
    "    x = tf.stack([image,segmentation], axis=-1) # stack image and segmentation state at the channel axis\n",
    "    y = tf.one_hot(mask, depth=2, dtype=tf.float32) # one hot encode binary mask to float tensor\n",
    "    y = tf.clip_by_value(y, 0.05, 0.95) # use soft labels to prevent nummerical issues during training\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Crop Mask\n",
    "def crop_mask(x, y, mask_size=(132,132,132)):\n",
    "    # apply crop after batch dimension is added x and y have (b,x,y,z,c) format while mask size has (x,y,z) format => add offset of 1\n",
    "    crop = [(y.shape[d+1]-mask_size[d])//2 for d in range(3)]\n",
    "    #keras implicitly assumes channels last format\n",
    "    y = tf.keras.layers.Cropping3D(cropping=crop)(y)\n",
    "    return x, y\n",
    "\n",
    "# chain dataset transformations to construct the input pipeline for training\n",
    "# 1. preprocess the raw data by clipping and normalizing the image. The mask is binarized and one hot encoded. A new axis is added to the image tensors -> (x,y,z,c) format\n",
    "# 2. Apply data augumentation if desired. Random affine transformations and elastic transformations can be applied to both tensors. This mapping operation can be parallelized to increase throughput as this is the most time consuming step in preprocessing. The number of parallel calls is automatically tuned for performance. \n",
    "# 3. Some training examples are prefetched which decouples preprocessing from model execution. The number of prefetched samples is tuned automatically for performance. \n",
    "\n",
    "trainingset = trainingset_raw.map(preprocessScene).map(utilities.tf_affine).prefetch(1) # x-> (x,y,z) y-> (x,y,z)\n",
    "testset = testset_raw.map(preprocessScene).map(utilities.tf_affine).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image mask pairs to Flood Filler instances\n",
    "def parseScene(x: tf.Tensor, y: tf.Tensor) -> floodFill.FloodFiller:\n",
    "    floodFill.constructExample(x, y, output_shape, input_shape, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting the Unet architecture to the Flood Filling algorithm\n",
    "\n",
    "In order for the Unet to be compatible to the Flood Filling algorithm, a few adaptations need to be made:\n",
    "- The Unet should directly output a probability map for an object to belong to the neuron -> add a softmax layer so that each channel is a pseudoprobability map for it's respective class\n",
    "- To prevent nummerical issues, 0.05 and 0.95 are used as target probabilities for the background or foreground respectively. Binary masks should be preprocessed accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<module 'model' from '../3D Unet\\\\model.py'>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model predicts softmax pseudoprobabilities\n"
    }
   ],
   "source": [
    "unet = model.build_unet(input_shape=(220,220,220,2), # Use two channels to feed image and segmentation canvas\n",
    "                        n_blocks=2,\n",
    "                        initial_filters=4,\n",
    "                        useSoftmax = True # Add a softmax layer to get pseudoprobability maps in output channels\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_4\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to\n==================================================================================================\ninput_5 (InputLayer)            [(None, 220, 220, 22 0\n__________________________________________________________________________________________________\ninput_block_4 (InputBlock)      ((None, 108, 108, 10 1092        input_5[0][0]\n__________________________________________________________________________________________________\ndownsample_block_8 (DownsampleB ((None, 52, 52, 52,  5208        input_block_4[0][0]\n__________________________________________________________________________________________________\ndownsample_block_9 (DownsampleB ((None, 24, 24, 24,  20784       downsample_block_8[0][0]\n__________________________________________________________________________________________________\nbottleneck_block_4 (BottleneckB (None, 40, 40, 40, 6 115872      downsample_block_9[0][0]\n__________________________________________________________________________________________________\nupsample_block_8 (UpsampleBlock (None, 72, 72, 72, 3 118880      bottleneck_block_4[0][0]\n                                                                 downsample_block_9[0][1]\n__________________________________________________________________________________________________\nupsample_block_9 (UpsampleBlock (None, 136, 136, 136 29744       upsample_block_8[0][0]\n                                                                 downsample_block_8[0][1]\n__________________________________________________________________________________________________\noutput_block_4 (OutputBlock)    (None, 132, 132, 132 6946        upsample_block_9[0][0]\n                                                                 input_block_4[0][1]\n__________________________________________________________________________________________________\nsoftmax_3 (Softmax)             (None, 132, 132, 132 0           output_block_4[0][0]\n==================================================================================================\nTotal params: 298,526\nTrainable params: 298,526\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom training loop for flood filling Unet\n",
    "\n",
    "The code provided in the [keras tutorial](https://keras.io/guides/writing_a_training_loop_from_scratch/) on custom training loops is used as backbone to train the FFUnet. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE LOSS, METRICS and OPTIMIZER\n",
    "\n",
    "# Generate a callable loss function that expects y_pred to be probabilities\n",
    "loss_fn = model.weighted_categorical_crossentropy(class_weights=[1,40], fromLogits=False)\n",
    "\n",
    "# Set up metrics that should be tracked during training\n",
    "training_acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "training_iou = metrics.MeanIoU(num_classes=2)\n",
    "validation_acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "validation_iou = metrics.keras_IoU(num_classes=2)\n",
    "\n",
    "# Define an optimizer to update the model weights given the respective gradients of the loss function\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# DEFINE TRAINING STEP\n",
    "# Decorate as tf function to allow static computational graph compilation\n",
    "@tf.function\n",
    "# Execute this step on each training instance (data, labels)\n",
    "def train_step(x, y):\n",
    "    # Open a gradient tape to track derivates with respect to model weights\n",
    "    with tf.GradientTape() as tape:\n",
    "        # predict probability maps in training mode\n",
    "        y_pred = unet(x, training=True)\n",
    "        # Compute the loss \n",
    "        loss_value = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss_value, unet.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, unet.trainable_weights))\n",
    "    # Accumulate metrics\n",
    "    training_acc.update_state(y, y_pred)\n",
    "    training_iou.update_state(y, y_pred)\n",
    "\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TEST STEP\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    y_pred = unet(x, training=False)\n",
    "    validation_acc.update_state(y,y_pred)\n",
    "    validation_iou.update_state(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Announce epoch\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # NEW EPOCH \n",
    "    # Get e new iterator over the training set\n",
    "    train_it = iter(trainingset)\n",
    "\n",
    "    # Iterate over the scenes of the dataset.\n",
    "    for step, (image, mask) in enumerate(train_it):\n",
    "\n",
    "        # Create an initialized Flood Filler instance from the scene\n",
    "        ff = parseScene(image, mask)\n",
    "\n",
    "        # Evaluate the model on the training scene, until the queue of the flood filler is empty\n",
    "        while ff.hasNext():\n",
    "            # retrieve the input image, segmentation canvas and mask of the tile that should be evaluated\n",
    "            im, seg, msk = ff.getNext()\n",
    "            # preprocess the tile to get unet input\n",
    "            x, y = preprocessTile(im,seg,msk)\n",
    "            # perform a training step\n",
    "            loss_value = train_step(x, y)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 5 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * 5))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    # get a new iterator over the validation set\n",
    "    val_it = iter(testset)\n",
    "    for image, mask in val_it:\n",
    "\n",
    "        # Create an initialized Flood Filler instance from the scene\n",
    "        ff = parseScene(image, mask)\n",
    "        # Run the unet on the scene and accumulate validation metrics\n",
    "        while ff.hasNext():\n",
    "            # retrieve the input image, segmentation canvas and mask of the tile that should be evaluated\n",
    "            im, seg, msk = ff.getNext()\n",
    "            # preprocess the tile to get unet input\n",
    "            x, y = preprocessTile(im,seg,msk)\n",
    "            # perform a test step\n",
    "            test_step(x, y)\n",
    "\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  }
 ]
}